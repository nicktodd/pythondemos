{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"deletable": false,
				"editable": false,
				"trusted": true
			},
			"source": [
				"\n",
				"# Glue Studio Notebook\n",
				"You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
				"\n",
				"## Available Magics\n",
				"|          Magic              |   Type       |                                                                        Description                                                                        |\n",
				"|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
				"| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
				"| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
				"| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
				"| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
				"| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
				"| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
				"| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
				"| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
				"| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
				"| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
				"| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
				"| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n",
				"| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
				"| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
				"| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
				"| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
				"| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
				"| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
				"| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
				"| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Installed kernel version: 0.35 \n",
						"Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::429937773353:role/AWS-Glue-S3-Bucket-Access\n",
						"Trying to create a Glue session for the kernel.\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 5\n",
						"Session ID: 5e282089-a999-4f84-8b14-27409ef2cf4a\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 0.35\n",
						"--enable-glue-datacatalog true\n",
						"Waiting for session 5e282089-a999-4f84-8b14-27409ef2cf4a to get into ready status...\n",
						"Session 5e282089-a999-4f84-8b14-27409ef2cf4a has been created\n",
						"\n",
						"\n"
					]
				}
			],
			"source": [
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"\n",
				"\n",
				"data_bucket_url = \"s3://spark.demo.data/\"\n",
				"lab_output_bucket_url = \"s3://spark.labs.saves/\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Exercise 1 Using the PySpark API\n",
				"\n",
				"Add some code in the notebook as follows:\n",
				"\n",
				"* Open the Macbeth.txt file from the lab bucket which is called spark-lab-data. This will return an RDD of strings, ie all the lines in the file. Assign the result to a variable named lines.\n",
				"\n",
				"To do this you will need to use the glueContext variable since this has the API capability to read and write directly to and from S3. The API call will be something like:\n",
				"\n",
				"```\n",
				"lines = glueContext.read.text(\"s3://spark-lab-data/Macbeth.txt\")\n",
				"```\n",
				"\n",
				"* Call lines.count() to count the number of items in the RDD (i.e. the number of lines in the file). Print the result.\n",
				"* Call lines.first() to obtain the first item in the RDD (i.e. the first line in the file). Print the result.\n",
				"* Call lines.filter() to filter the lines, and print the result. Note that the filter() function takes the column (which in this case is called value) and then a contains() function can be used to filter out the content. So the code would look something like:\n",
				"\n",
				"```\n",
				"witchLines = lines.filter(lines[\"value\"].contains(\"Witch\"))\n",
				"```\n",
				"Run your code block and you should see output like the following\n",
				"* Number of lines: 4102\n",
				"* First line: ACT I\n",
				"* Witch lines: (A collection of RDD objects)\n",
				"\n",
				" \n",
				"## Saving RDDs \n",
				"Modify the code so that it saves the witchLines RDD which you then can write to S3. \n",
				"\n",
				"The easiest way to complete this is to use the write function contained within the dataframe itself. Save the file to your S3 bucket with a path something like YOURBUCKET/lab1. \n",
				"\n",
				"The code would be something like this but the bucket name would be different.\n",
				"\n",
				"```\n",
				"witchLines.write.format('csv').option('header','false').save('s3://glue-nickt/lab1')\n",
				"```\n",
				"\n",
				"Now go to the S3 service in AWS and locate your bucket. There should now be a new file in there with your outpput in it. If you want to, you can download it and review the contents.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# add code for exercise 1 here\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Exercise 2 RDD Operations Part 1\n",
				"\n",
				"## Overview\n",
				"In this lab you'll use various RDD transformation methods to manipulate data from several simple text files.\n",
				"\n",
				"## Roadmap\n",
				"There are 3 exercises in this lab, of which the last exercise is \"if time permits\". Here is a brief summary of the tasks you will perform in each exercise; more detailed instructions follow later:\n",
				"1. Mapping and filtering data\n",
				"2. Performing set-based operations\n",
				"3. (If Time Permits) Additional transformation suggestions\n",
				"\n",
				"## Part 1: Mapping and filtering data\n",
				"In this exercise you will perform various mapping and filtering operations on RDD objects. You will do all your work in app1.py, so open this file in the text editor now. The application makes use of data from two text files:\n",
				"* klm.txt ‚Äì contains a partial list of airports that KLM flies into.\n",
				"* norwegian.txt ‚Äì contains a partial list of airports that Norwegian Airlines flies into.\n",
				"\n",
				"Follow the comments in the codeblock below to complete this exercise. After each step, save the Notebook and use the play button to test. Make sure you have run the first code block in the notebook first to ensure that the glueContext and sparkContext is all configured."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n",
						"KLM airports in uppercase: ['LONDON HEATHROW', 'LONDON GATWICK', 'AMSTERDAM', 'CARDIFF', 'OSLO', 'DUBAI', 'SINGAPORE', 'BERGEN', 'BRISTOL', 'PARIS', 'LONDON CITY', 'LUTON', 'LIVERPOOL']\n",
						"\n",
						"KLM airports that start with 'L': None\n",
						"\n",
						"Union of all airports: None\n",
						"\n",
						"All distinct airports: None\n",
						"\n",
						"Airports in common: None\n",
						"\n",
						"Airports served by KLM but not Norwegian: None\n"
					]
				}
			],
			"source": [
				"klmAirports = glueContext.read.text(data_bucket_url + \"klm.txt\").rdd\n",
				"norAirports = glueContext.read.text(data_bucket_url + \"norwegian.txt\").rdd\n",
				"\n",
				"\n",
				"# In each of the following statements, replace \"None\" with a suitable call to a PySpark API function...\n",
				"# We've done the first one for you, to get you started...\n",
				"\n",
				"# Get KLM airports in uppercase.\n",
				"klmUpperCaseAirports = klmAirports.map(lambda airport: airport[\"value\"].upper()).collect()\n",
				"print(\"\\nKLM airports in uppercase: %s\" % klmUpperCaseAirports)\n",
				"\n",
				"# Get KLM airports that start with \"L\" (hint, Python strings have a startswith() method).\n",
				"klmLAirports = None\n",
				"print(\"\\nKLM airports that start with 'L': %s\" % klmLAirports)\n",
				"\n",
				"# Get the union of all airports.\n",
				"allAirports = None\n",
				"print(\"\\nUnion of all airports: %s\" % allAirports)\n",
				"\n",
				"# Get all distinct airports.\n",
				"distinctAirports = None\n",
				"print(\"\\nAll distinct airports: %s\" % distinctAirports)\n",
				"\n",
				"# Get airports in common.\n",
				"commonAirports = None\n",
				"print(\"\\nAirports in common: %s\" % commonAirports)\n",
				"\n",
				"# Get airports served by KLM but not Norwegian.\n",
				"klmOnlyAirports = None\n",
				"print(\"\\nAirports served by KLM but not Norwegian: %s\" % klmOnlyAirports)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 2: Performing set-based operations\n",
				"In this exercise you will perform set-based operations on RDD objects. You will do all your work in the code block below. The application makes use of data from two text files:\n",
				"\n",
				"* teams.txt ‚Äì contains the names of Premier League teams, 2017-18 (happy days!)\n",
				"* stadiums.txt ‚Äì contains the stadiums of Premier League teams, 2017-18\n",
				"\n",
				"Follow the comments in the code block below to complete this exercise. After each step, save the Notebook and click the play button (remembering to ensure that the block at the top of the notebook has executed. \n",
				"\n",
				"Here are some additional hints:\n",
				"* To zip teams with stadiums, use the zip() method. You should obtain a collection of tuples such as (Arsenal, The Emirates), (Bournemouth, Vitality Stadium), etc.\n",
				"* To get the Cartesian product of all teams, call the cartesian() method on the teams RDD. Also pass the teams RDD as a parameter, so that you perform a Cartesian product between all the teams. You should obtain a collection of tuples such as (Arsenal, Arsenal), (Arsenal, Bournemouth), (Arsenal, Burnley), etc.\n",
				"* To get all the fixtures, it's similar to getting a Cartesian product but you need to filter-out the tuples where element [0] equals element [1]. E.g., tuples such as (Arsenal, Arsenal) should be excluded from the result. \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Teams and stadiums: None\n",
						"Cartesian product of all teams: None\n",
						"Fixtures: None\n"
					]
				}
			],
			"source": [
				"teams = glueContext.read.text(data_bucket_url + \"teams.txt\").rdd\n",
				"stadiums = glueContext.read.text(data_bucket_url + \"stadiums.txt\").rdd\n",
				"\n",
				"# In each of the following statements, replace \"None\" with a suitable call to a PySpark API function...\n",
				"\n",
				"# Zip teams with stadiums.\n",
				"teamsAndStadiums = None\n",
				"print(\"Teams and stadiums: %s\" % teamsAndStadiums)\n",
				"\n",
				"# Cartesian product of all teams.\n",
				"cartesian = None\n",
				"print(\"Cartesian product of all teams: %s\" % cartesian)\n",
				"\n",
				"# Fixtures.\n",
				"fixtures = None\n",
				"print(\"Fixtures: %s\" % fixtures)\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 3 (If time permits):  Additional transformation suggestions \n",
				"In this exercise you'll perform various transformation operations on data for some chemical elements in the periodic table. The data is located in elements.txt. For each element, we've specified the following information (we've used tabs as field separators):\n",
				"* Atomic number\n",
				"* Atomic mass\n",
				"* Group number in the periodic table\n",
				"* Period number in the periodic table\n",
				"* Symbol \n",
				"* Name of element\n",
				"\n",
				"You can review the periodic table here if you are a bit rusty on your chemistry!\n",
				"https://sciencenotes.org/wp-content/uploads/2013/06/PeriodicTable-NoBackground2.png\n",
				"\n",
				"We've defined a Python class named Element in the codeblock below to represent an element in the periodic table.  All the code you have to write will be in the block below that. Complete the application as per the comments in codeblock below the Element block. \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# Run  this block once to load the class\n",
				"class Element:\n",
				"    def __init__(self, str):\n",
				"        arr = str.split(\"\\t\") \n",
				"        self.atomicNumber = int(arr[0])\n",
				"        self.atomicMass = float(arr[1])\n",
				"        self.group = int(arr[2])\n",
				"        self.period = int(arr[3])\n",
				"        self.symbol = arr[4]\n",
				"        self.name = arr[5]\n",
				"\n",
				"    def __repr__(self):\n",
				"        return \"%d | %f | %d | %d | %s | %s\" % (self.atomicNumber, self.atomicMass, self.group, self.period, self.symbol, self.name)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"lines = glueContext.read.text(data_bucket_url + \"elements.txt\").rdd\n",
				"\n",
				"# In each of the following statements, replace \"None\" with a suitable call to a PySpark API function...\n",
				"\n",
				"# Map each line into an Element object. \n",
				"elements = None\n",
				"                        \n",
				"# Group elements by period.\n",
				"groupedByPeriod = None\n",
				"print(\"Elements grouped by period: %s\" % groupedByPeriod)\n",
				"\n",
				"# Create a dict, where the key is the element symbol, and the value is the element itself.\n",
				"keyedBySymbol = None\n",
				"print(\"Elements keyed by symbol: %s\" % keyedBySymbol)\n",
				"\n",
				"# Sort elements by name.\n",
				"sortedByName = None\n",
				"print(\"Elements sorted by name: %s\" % sortedByName)\n",
				"\n",
				"# Repartition elements into 5 partitions, and then add code to save in a directory named \"partitionedElements\".\n",
				"repartitionedElements = None"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Exercise 3 RDD Operations Part 2\n",
				"\n",
				"## Part 1: Mapping lines of text to a flat map of words\n",
				"In the empty code box below. create an RDD named lines, containing all the lines of text from MacbethSnippet.txt which is a file in the labfile bucket used in the other code blocks.\n",
				"\n",
				"Where indicated by the \"Ex 1\" comment, add code to create an RDD containing all the words in all the lines. Here are some hints:\n",
				"* Use the RDD flatMap() function, whose purpose is to map an item into a sequence of sub-items. In this case, it will map a line into its constituent words‚Ä¶\n",
				"* flatMap() takes a lambda expression as a parameter, indicating how to map each item into sub-items. Implement the lambda so that it splits a line at the space character (call the split(' ') function to do this).\n",
				"* The flatMap() function returns all words, including words that are effectively empty. Call filter() to just keep the non-empty words. You'll need to write a lambda that tests if a word isn't empty‚Ä¶ how can you test if a Python string isn't empty?\n",
				"\n",
				"Finally, print the resultant RDD to the console, via the print() function. In order to see the actual contents of the RDD, call collect() on the RDD for simplicity.\n",
				"\n",
				"Run the code block to verify that the application prints all the words found in the text file. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add your code for part 1 here.\n",
				"lines = glueContext.read.text(data_bucket_url + \"MacbethSnippet.txt\").rdd\n",
				"\n",
				"# Create RDD of words using flatMap and filter\n",
				"words = None\n",
				"\n",
				"print(\"All words: %s\" % words.collect())\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part  2: Creating an RDD of (word, count) tuples\n",
				"In the following code block, add code to create an RDD containing tuples of words and word counts (i.e. the first item in the tuple is a word, and the second item is the count of how many times that word appears). \n",
				"\n",
				"Follow these steps:\n",
				"* Call map() to map each word into a (word, count) tuple where each count is 1 initially. \n",
				"* Call reduceByKey(), to group (key, value) tuples by key and then reduce the values into a single value. E.g. imagine you start off with the following (word, count) tuples:\n",
				"    * (\"do\", 1)\n",
				"    * (\"you\", 1)\n",
				"    * (\"know\", 1)\n",
				"    * (\"the\", 1)\n",
				"    * (\"muffin\", 1)\n",
				"    * (\"man\", 1)\n",
				"    * (\"the\", 1)\n",
				"    * (\"muffin\", 1)\n",
				"    * (\"man\", 1)\n",
				"    *  (\"the\", 1)\n",
				"    * (\"muffin\", 1)\n",
				"    * (\"man\", 1)\n",
				"\n",
				"reduceByKey() first groups these tuples by key (i.e. word) as follows:\n",
				"    * (\"do\", 1)\n",
				"    * (\"you\", 1)\n",
				"    * (\"know\", 1)\n",
				"    * (\"the\", 1)\t(\"the\", 1)\t(\"the\", 1)\n",
				"    * (\"muffin\", 1)\t(\"muffin\", 1) \t(\"muffin\", 1)\n",
				"    * (\"man\", 1)\t(\"man\", 1)\t(\"man\", 1)\n",
				"\n",
				"For each different key, reduceByKey() calls the lambda expression that you provide, successively with each value. E.g. for the \"muffin\" key, it'll call your lambda 3 times, passing the values 1, 1, 1 each time. \n",
				"\n",
				"So, implement a lambda that accumulates the total word count for each word. The lambda receives two parameters: \n",
				"* The accumulated value so far (defaults to 0 on the first call)\n",
				"* The value of the next item in the key group (e.g. the value 1)\n",
				"* Finally call sortBy() to sort the (word, count) tuples by descending word count.\n",
				"* Print the resultant RDD of (word, count) tuples.\n",
				"\n",
				"Test your new code. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add your code for Part 2 here"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 3: Caching the RDD of (word, count) tuples\n",
				"\n",
				"The exercises that follow will perform various actions on the RDD of (word, count) tuples. Under normal circumstances, Spark would re-evaluate all the intermediate steps to recreate the RDD ready for each action. This is inefficient, so add code to cache the RDD of (word, count) tuples before going any further."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"# add code for part 3"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"\n",
				"## Part 4: Displaying the first 100 words\n",
				"\n",
				"In the code box below, add code to display the first100 words. Here are some hints:\n",
				"* First call take() to create an RDD containing just the first 100 items.\n",
				"* Then pass the resultant RDD to the Python print() function.\n",
				"\n",
				"---\n",
				"**NOTE**\n",
				"Aside: An alternative to take() is collect(). However, collect() might cause the Spark driver to run out of memory, because it fetches the entire RDD to a single machine. This is why take() is generally a safer option, because it limits the number of items being collated.\n",
				"\n",
				"---"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 4 here\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 5: Performing aggregation actions\n",
				"\n",
				"In the code block below, add code to perform the following aggregation actions on the RDD of (word, count) tuples:\n",
				"* Find the count of all items in the RDD, via the RDD count() method. This tells you the number of different words in MacbethSnippet.txt. \n",
				"* Find the most frequent word. Use the RDD max() method to do this. Note the following points about the max() method:\n",
				"    * The max() method takes an optional lambda expression, which allows you to specify how to compare items‚Ä¶ \n",
				"    * In our scenario, the RDD contains (word, count) tuples, and you want max() to compare element [1] in the tuples (i.e. the counts)‚Ä¶\n",
				"    * Therefore, when you call max(), pass in a lambda that takes a (word, count) tuple and returns element [1] from the tuple. \n",
				"* In the same way, find the least frequent word. Use the RDD min() method to do this."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"# add code for part 5\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"## Part 6 (If time permits): Performing key-based actions\n",
				"In the code block below, add code to lookup a word and find its count. \n",
				"\n",
				"You could for example set up a list of words that you would like to count.\n",
				"\n",
				"Write code to iterate through the list. For each word, look it up in the (word, count) tuple. This will tell you the occurrence count for that word. Display the results."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add the code for part 6 here\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 7 (If time permits): Performing numeric actions\n",
				"\n",
				"In the code block below, add code to perform the following numeric actions on the word counts:\n",
				"* The sum of all counts (i.e. the total number of words)\n",
				"* The average (mean) of all counts\n",
				"* The standard deviation of all counts\n",
				"* The variance of all counts\n",
				"\n",
				"Note 1: You'll first need to create an RDD containing just the word counts but not the words themselves ‚Äì how will you do this?\n",
				"Note 2: Ensure that the process by which you map (word, count) tuples isn't repeated each time you invoke one of the action methods.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 7 here\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Exercise 6 Getting Started with Spark SQL\n",
				"\n",
				"## Overview\n",
				"In this lab you‚Äôll use Spark SQL to read data from a CSV file. This is very similar to reading data from a JSON file, which we covered during the chapter.\n",
				"\n",
				"Roadmap\n",
				"There are 5 exercises in this lab, of which the last exercise is \"if time permits\". Here is a brief summary of the tasks you will perform in each exercise; more detailed instructions follow later:\n",
				"1. Reading data from a CSV file into a DataFrame\n",
				"2. Specifying options when reading a file\n",
				"3. Defining a schema for the data\n",
				"4. Executing a SQL query\n",
				"5. (If Time Permits) Generating computed columns\n",
				" \n",
				"## Familiarization\n",
				"From the lab bucket, download and review weather.csv in a text editor. This file contains real temperature and precipitation measurements for every day in Bergen in 2019 (spoiler alert, it was wet). All weather measurements are from Yr, delivered by the Norwegian Meteorological Institute and NRK. \n",
				"\n",
				"The CSV file has 365 rows, containing the data for every day from 1 January to 31 December. Each row has 4 values:\n",
				"* The first value indicates the day number in a month (notice how this value wraps from 31 back to 1 as we move from January to February, for example).\n",
				"* The next value contains the minimum temperature for a day, in degrees Celsius.\n",
				"* The next value contains the maximum temperature for a day, in degrees Celsius.\n",
				"* The final value contains the precipitation for a day, in mm.\n",
				" \n",
				"## Part 1: Reading data from a CSV file into a DataFrame\n",
				"In the below code block add code to read data from weather.csv into a Spark SQL DataFrame object in memory. It‚Äôs similar to how you read JSON (see the PowerPoint chapter for a reminder), except you call the csv() method instead:\n",
				"```\n",
				"df = glueContext.read.csv(\"s3://spark-lab-data/weather.csv\")\n",
				"```\n",
				"\n",
				"Spark SQL parses each line of text in the file, using a comma as the default delimiter. It creates a DataFrame with 365 rows, with 4 columns per row. Display the DataFrame object as follows:\n",
				"```\n",
				"df.show()\n",
				"```\n",
				"\n",
				"Run the code as usual:\n",
				"  \n",
				"Note that the DataFrame‚Äôs columns are named _c0, _c1, _c2, and _c3 by default:"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 1\n",
				"df = glueContext.read.csv(data_bucket_url + \"weather.csv\")\n",
				"df.show()\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"   \n",
				"## Part 2: Specifying options when reading a file\n",
				"In the previous exercise your Python application loaded weather.csv into a DataFrame. The CSV file didn‚Äôt give any indication about the meaning of each field, so Spark SQL created columns named _c0, _c1, _c2, and _c3 by default.\n",
				"\n",
				"An alternative approach is for the CSV file to include a ‚Äúheader line‚Äù at the top. We‚Äôve done this in weatherWithHeader.csv ‚Äì download this file from the S3 bucket and take a look in a text editor. Also note that the file happens to use semi-colons rather than commas as the field delimiter, just to make life interesting.\n",
				"\n",
				"In order to read this file into a DataFrame, you have to specify a couple of options to PySpark:\n",
				"* You need to tell PySpark to expect the header line.\n",
				"* You need to tell PySpark that the field delimiter is a semi-colon rather than a comma.\n",
				"\n",
				"This is how you specify options when reading a CSV file: \n",
				"```\n",
				"df = glueContext.read \\\n",
				"                  .option(option-name-1, option-value-2) \\\n",
				"                  .option(option-name-2, option-value-2) \\\n",
				"                  .csv(\"s3://spark-lab-data/weatherWithHeader.csv\")\n",
				"```\n",
				"\n",
				"Take a look at the online docs here to see what option names and values you need to specify:\n",
				"https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
				"\n",
				"(Scroll down the website a couple of pages to see a list of all the option properties you can set. You‚Äôre looking for how to specify that a header line is expected, and that the delimiter is a semi-colon).\n",
				"When you‚Äôve modified your code, submit your application to PySpark again and verify it works properly. You should now see columnar output with proper column names."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 2\n",
				"df = glueContext.read \\\n",
				"               .option(\"delimiter\", \";\") \\\n",
				"               .option(\"header\", \"true\") \\\n",
				"               .csv(data_bucket_url + \"weatherWithHeader.csv\")\n",
				"               \n",
				"df.show()\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 3: Defining a schema for the data\n",
				"In your application as it stands, you don‚Äôt tell Spark SQL about the schema of the CSV data in advance. Therefore, Spark SQL has to scan the entire dataset initially so that it can infer the schema from the shape of the data.\n",
				"\n",
				"If you know the schema, you can tell the Spark SQL in advance by calling the schema() method on the DataFrameReader. Modify your code to do this, then run the application again to make sure it all still works fine. You won‚Äôt see any changes in your application‚Äôs behaviour, but under the covers you‚Äôve improved its efficiency üëç."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 3\n",
				"from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
				"\n",
				"weatherSchema = StructType([\n",
				"\tStructField(\"DayOfMonth\", IntegerType(), False),\n",
				"\tStructField(\"MinTemp\", DoubleType(), False),\n",
				"\tStructField(\"MaxTemp\", DoubleType(), False),\n",
				"\tStructField(\"Precipitation\", DoubleType(), False)\n",
				"])\n",
				"\n",
				"df = glueContext.read \\\n",
				"               .schema(weatherSchema) \\\n",
				"               .option(\"delimiter\", \";\") \\\n",
				"               .option(\"header\", \"true\") \\\n",
				"               .csv(data_bucket_url + \"weatherWithHeader.csv\")\n",
				"               \n",
				"df.show()\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 4: Executing a SQL query\n",
				"So far in the lab, you‚Äôve successfully managed to load CSV data into a DataFrame object. The whole reason for doing this is so that you can execute Spark SQL queries on the data. This is what you‚Äôll do now‚Ä¶\n",
				"\n",
				"Follow these steps:\n",
				"* On your DataFrame object, call the createOrReplaceTempView() method to create a temporary view in Hive MetaStore. This enables you to execute SQL statements on the data.\n",
				"* On your GlueContext object, call the sql() method to invoke some SQL. For example, select the DayOfMonth, MaxTemp, and MinTemp columns for each record. Display the results returned by the sql() method.\n",
				"\n",
				"Run your code and verify it works."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 4\n",
				"\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Part 5 (If Time Permits): Generating computed columns\n",
				"Spark SQL allows you to add computed columns to a DataFrame. For example, you could add a column named DiurnalRange that contains the difference between the maximum and minimum temperature for each day.\n",
				"\n",
				"One way to add a column to a DataFrame is to call the withColumn() method. Pass in two parameters as follows:\n",
				"* The name of the column you want to add, e.g., \"DiurnalRange\".\n",
				"* The value you want to compute, i.e., the difference between the MaxTemp column and the MinTemp column. You can access a column value via the col() function, e.g. col(\"MaxTemp\") will give you the value of the MaxTemp column. The col() function is defined in the pyspark.sql.functions module, so you‚Äôll need to import it.\n",
				"\n",
				"Display the DataFrame to verify that it now has a DiurnalRange column. Consider how you can improve the output, e.g. via the round() function in the pyspark.sql.functions module."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"# Add code for part 5 \n",
				"\n",
				"\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
